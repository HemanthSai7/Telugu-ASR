speech_config:
  sample_rate: 16000
  frame_ms: 25
  stride_ms: 10
  num_feature_bins: 80
  feature_type: log_mel_spectrogram
  preemphasis: 0.97
  pad_end: False
  lower_edge_hertz: 0.0
  upper_edge_hertz: 8000.0
  output_floor: 1e-9
  log_base: "10"
  nfft: 512
  normalize_signal: True
  normalize_zscore: False
  normalize_min_max: False
  padding: 0.0

model_config:
  name: asr_telugu_model
  d_model: 256
  subsampling_config:
    name: conv1d
    kernel_size: [9,9,9]
    strides: [2,2,2]
    padding: ["valid", "valid", "valid"]
    activation: ["gelu", "gelu", "gelu"]
  encoder_config:
    num_blocks: 4
    fc_factor: 1
    attention_type: mhsa
    num_heads: 8
    head_dim: 32
    activation: gelu
    dropout: 0.20822049870481718
  decoder_config:
    num_blocks: 5
    fc_factor: 1
    attention_type: mhsa
    num_heads: 8
    head_dim: 32
    activation: swiglu
    dropout: 0.054025138005453555
  kernel_regularizer:
    class_name: l2
    config:
      l2: 9.774938009391273e-05
  bias_regularizer:
    class_name: l2
    config:
      l2: 9.774938009391273e-05
  kernel_initializer:
    class_name: glorot_uniform
    config:
      seed: 42
  bias_initializer: zeros
  # dropout: 0.1
  # vocabulary: ../../../vocabulary/vocab.json

data_config:
  train_dataset_config:
    enabled: True
    data_paths:
      - /home/hemanth/GIT_Projects/RA/data/train.tsv
    shuffle: False
    cache: True
    buffer_size: 1000
    drop_remainder: True
    stage: train
    metadata: null
    indefinite: True

  eval_dataset_config:
    enabled: True
    data_paths:
      - /home/hemanth/GIT_Projects/RA/data/dev.tsv
    shuffle: True
    cache: True
    buffer_size: 1000
    drop_remainder: True
    stage: eval
    metadata: null
    indefinite: True

  test_dataset_config:
    enabled: True
    data_paths:
      - /home/hemanth/GIT_Projects/RA/data/test.tsv
    shuffle: False
    cache: True
    buffer_size: 1000
    drop_remainder: False
    stage: test
    indefinite: False

learning_config:
  optimizer_config:
    class_name: Adam
    config:
      learning_rate:
        class_name: src.optimizers.schedules>TransformerLearningRateSchedule
        config:
          d_model: 256
          warmup_steps: 11694
          max_lr: 0.0001993592174132318
          min_lr: 1.8488588312593742e-06
      beta_1: 0.9
      beta_2: 0.98
      epsilon: 1e-9
  
  pretrained: False

  running_config:
    batch_size: 8
    num_epochs: 100
    devices: [0]
    checkpoint:
      filepath: checkpoints/{epoch:02d}.h5
      save_best_only: False
      save_weights_only: True
      save_freq: epoch
    states_dir: states
    csv_logger: training.log
    tensorboard:
      log_dir: tensorboard
      histogram_freq: 1
      write_graph: True
      write_images: True
      update_freq: epoch
      profile_batch: 2
