{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a3b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, IterableDataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8655de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"../data/IISc_RESPIN_train_te_small/meta_train_te_small.json\").T\n",
    "train = train[[\"wav_path\", \"duration\", \"text\"]].reset_index(drop=True)\n",
    "# train[\"wav_path\"] = train[\"wav_path\"].apply(lambda x: \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_train_te_small/\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8aafb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1438, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = pd.read_json(\"/home/hemanth/GIT_Projects/Telugu-ASR/data/dev.tsv\").T\n",
    "# dev = dev[[\"wav_path\", \"duration\", \"text\"]].reset_index(drop=True)\n",
    "# dev[\"wav_path\"] = dev[\"wav_path\"].apply(lambda x: \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_dev_te/\" + x)\n",
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7e9a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2226, 17)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_json(\"/home/hemanth/GIT_Projects/Telugu-ASR/data/test.tsv\").T\n",
    "# test = test[[\"wav_path\", \"duration\", \"text\"]].reset_index(drop=True)\n",
    "# test[\"wav_path\"] = test[\"wav_path\"].apply(lambda x: \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_test_te/\" + x)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a020f047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155.8890627256954, 2.3021305555555562, 3.3747861111111024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"duration\"].sum() / 3600, dev[\"duration\"].sum() / 3600, test[\"duration\"].sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ca5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['é']\n",
      "['0xe9']\n",
      "['e', '́']\n",
      "['0x65', '0x301']\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# import unicodedata\n",
    "\n",
    "# s = \"é\"\n",
    "# print(list(s))  # ['é']\n",
    "# print([hex(ord(c)) for c in s])  # ['0xe9']\n",
    "\n",
    "# s2 = \"é\"  # looks the same, but actually two codepoints\n",
    "# print(list(s2))  # ['e', '́']\n",
    "# print([hex(ord(c)) for c in s2])  # ['0x65', '0x301']\n",
    "\n",
    "# print(s == s2)  # False\n",
    "\n",
    "# # Normalize\n",
    "# s_norm = unicodedata.normalize(\"NFC\", s2)\n",
    "# print(s == s_norm)  # True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f2cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_dev_te/meta_dev_te.json\"\n",
    "output_file = \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_dev_te/meta_dev_te.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# print(data)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for item in data.values():\n",
    "        # print(item)\n",
    "        # break\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f14b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! TSV file saved as: /home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_train_te_small/output.tsv\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import csv\n",
    "\n",
    "# def convert_jsonl_to_tsv(input_file, output_file):\n",
    "#     \"\"\"\n",
    "#     Convert JSONL file to TSV with columns: PATH, DURATION, TRANSCRIPT\n",
    "    \n",
    "#     Args:\n",
    "#         input_file (str): Path to input JSONL file\n",
    "#         output_file (str): Path to output TSV file\n",
    "#     \"\"\"\n",
    "    \n",
    "#     with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "#          open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        \n",
    "#         # Create TSV writer\n",
    "#         writer = csv.writer(outfile, delimiter='\\t')\n",
    "        \n",
    "#         # Write header\n",
    "#         writer.writerow(['PATH', 'DURATION', 'TRANSCRIPT'])\n",
    "        \n",
    "#         # Process each line in JSONL\n",
    "#         for line_num, line in enumerate(infile, 1):\n",
    "#             line = line.strip()\n",
    "#             if not line:  # Skip empty lines\n",
    "#                 continue\n",
    "                \n",
    "#             try:\n",
    "#                 # Parse JSON\n",
    "#                 data = json.loads(line)\n",
    "                \n",
    "#                 # Extract required fields\n",
    "#                 path = data.get('wav_path', '')\n",
    "#                 duration = data.get('duration', '')\n",
    "#                 transcript = data.get('text', '')\n",
    "                \n",
    "#                 # Write row to TSV\n",
    "#                 writer.writerow([path, duration, transcript])\n",
    "                \n",
    "#             except json.JSONDecodeError as e:\n",
    "#                 print(f\"Error parsing JSON on line {line_num}: {e}\")\n",
    "#                 continue\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing line {line_num}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     input_file = \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_train_te_small/meta_train_te_small.jsonl\"  # Replace with your input file path\n",
    "#     output_file = \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_train_te_small/output.tsv\"  # Replace with your desired output file path\n",
    "\n",
    "#     convert_jsonl_to_tsv(input_file, output_file)\n",
    "#     print(f\"Conversion complete! TSV file saved as: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd440909",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/hemanth/GIT_Projects/Telugu-ASR/data/train.tsv\", sep=\"\\t\")\n",
    "valid = pd.read_csv(\"/home/hemanth/GIT_Projects/Telugu-ASR/data/dev.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"/home/hemanth/GIT_Projects/Telugu-ASR/data/test.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac79f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# train[\"PATH\"] = train[\"PATH\"].apply(lambda x: \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_train_te_small/\" + x)\n",
    "# valid[\"PATH\"] = valid[\"PATH\"].apply(lambda x: \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_dev_te/\" + x)\n",
    "# test[\"PATH\"] = test[\"PATH\"].apply(lambda x: \"/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_test_te/\" + x)\n",
    "\n",
    "train[\"TRANSCRIPT\"] = train[\"TRANSCRIPT\"].apply(lambda x: unicodedata.normalize(\"NFC\", x))\n",
    "valid[\"TRANSCRIPT\"] = valid[\"TRANSCRIPT\"].apply(lambda x: unicodedata.normalize(\"NFC\", x))\n",
    "test[\"TRANSCRIPT\"] = test[\"TRANSCRIPT\"].apply(lambda x: unicodedata.normalize(\"NFC\", x))\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_specific_punct(text):\n",
    "    # Remove only , ? '\n",
    "    return re.sub(r\"[,\\?']\", \"\", text)\n",
    "\n",
    "train[\"TRANSCRIPT\"] = train[\"TRANSCRIPT\"].apply(remove_specific_punct)\n",
    "valid[\"TRANSCRIPT\"] = valid[\"TRANSCRIPT\"].apply(remove_specific_punct)\n",
    "test[\"TRANSCRIPT\"] = test[\"TRANSCRIPT\"].apply(remove_specific_punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f98b5f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATH</th>\n",
       "      <th>DURATION</th>\n",
       "      <th>TRANSCRIPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>10.81</td>\n",
       "      <td>కరెంటు బిల్లుల్ని కూడా సులువుగా ఆండ్రాయిడ్ ఫోన...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>7.44</td>\n",
       "      <td>వైద్యపరంగా ఉసిరికలో ఔషధగుణాలెక్కువంట  అందుకే ర...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>6.71</td>\n",
       "      <td>ఫైనాన్స్లో  రుణం అనేది బాండ్ల జారీ ద్వారా సేకర...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>8.41</td>\n",
       "      <td>పాడి పశువులకు గాలికొంటా టీకా డోస్ ని సకాలంలో ఇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>8.73</td>\n",
       "      <td>లేత చిక్కుడులో గింజలుండవు అలానే తొక్కలలో మాత్ర...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95275</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>7.67</td>\n",
       "      <td>అరెకరంలోనే మనకు కావలసిన అన్ని రకాల పంటలను అన్న...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95276</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>7.68</td>\n",
       "      <td>మన సిత్తూరు జిల్లా రకరకాల ఉద్యానవనాలకు పెట్టిం...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95277</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>5.88</td>\n",
       "      <td>లేగదూడ పుట్టి మూడు దినాలైనంక ఎక్కువ పాలు తాగని...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95278</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>6.69</td>\n",
       "      <td>మా ప్రెసిడెంట్ కారుని లారీ గుద్దేశినాదని ఇన్సూ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95279</th>\n",
       "      <td>/home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...</td>\n",
       "      <td>8.89</td>\n",
       "      <td>పాతరోజుల్లో గుర్రాలు  గాడిదలు  ఎద్దులు మొదలైన ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95280 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    PATH  DURATION  \\\n",
       "0      /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...     10.81   \n",
       "1      /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...      7.44   \n",
       "2      /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...      6.71   \n",
       "3      /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...      8.41   \n",
       "4      /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...      8.73   \n",
       "...                                                  ...       ...   \n",
       "95275  /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...      7.67   \n",
       "95276  /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...      7.68   \n",
       "95277  /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...      5.88   \n",
       "95278  /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...      6.69   \n",
       "95279  /home/hemanth/GIT_Projects/Telugu-ASR/data/IIS...      8.89   \n",
       "\n",
       "                                              TRANSCRIPT  \n",
       "0      కరెంటు బిల్లుల్ని కూడా సులువుగా ఆండ్రాయిడ్ ఫోన...  \n",
       "1      వైద్యపరంగా ఉసిరికలో ఔషధగుణాలెక్కువంట  అందుకే ర...  \n",
       "2      ఫైనాన్స్లో  రుణం అనేది బాండ్ల జారీ ద్వారా సేకర...  \n",
       "3      పాడి పశువులకు గాలికొంటా టీకా డోస్ ని సకాలంలో ఇ...  \n",
       "4      లేత చిక్కుడులో గింజలుండవు అలానే తొక్కలలో మాత్ర...  \n",
       "...                                                  ...  \n",
       "95275  అరెకరంలోనే మనకు కావలసిన అన్ని రకాల పంటలను అన్న...  \n",
       "95276  మన సిత్తూరు జిల్లా రకరకాల ఉద్యానవనాలకు పెట్టిం...  \n",
       "95277  లేగదూడ పుట్టి మూడు దినాలైనంక ఎక్కువ పాలు తాగని...  \n",
       "95278  మా ప్రెసిడెంట్ కారుని లారీ గుద్దేశినాదని ఇన్సూ...  \n",
       "95279  పాతరోజుల్లో గుర్రాలు  గాడిదలు  ఎద్దులు మొదలైన ...  \n",
       "\n",
       "[95280 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "361bb693",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"/home/hemanth/GIT_Projects/Telugu-ASR/data/train_final.tsv\", sep=\"\\t\", index=False)\n",
    "valid.to_csv(\"/home/hemanth/GIT_Projects/Telugu-ASR/data/dev_final.tsv\", sep=\"\\t\", index=False)\n",
    "test.to_csv(\"/home/hemanth/GIT_Projects/Telugu-ASR/data/test_final.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3720f3d9",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "So, at the initial glance, I didn't knew what the D* folders meant. It was after I read the paper shared in the mail that I understood they were different dialects.\n",
    "\n",
    "#### Steps taken:\n",
    "1. Tried training a new tokenizer based on the documentation in huggingface. The ideal vocab size (atleast 32000 in whisper) is too large to fit on my laptop.\n",
    "2. Went ahead with a character-level tokenizer implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1501310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = load_dataset(\"json\", data_files=\"data/IISc_RESPIN_train_te_small/meta_train_te_small.jsonl\", split=\"train\")\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "dev_dataset = Dataset.from_pandas(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efad2657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wav_path': '/home/hemanth/GIT_Projects/RA/data/IISc_RESPIN_train_te_small/D1/90001/IISc_RESPIN_te_D1_90001_517297_M_BANK_900548_90000002.wav', 'duration': 10.81, 'text': 'కరెంటు బిల్లుల్ని కూడా సులువుగా ఆండ్రాయిడ్ ఫోన్ ఉంటే జీపే ద్వారా చెయ్యొచ్చు'}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8491d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(\"Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0\")\n",
    "# tokenizer = old_tokenizer.train_new_from_iterator(\n",
    "#     (row[\"text\"] for row in train_dataset), 10000\n",
    "# )\n",
    "\n",
    "example = \"కరెంటు బిల్లుల్ని కూడా సులువుగా ఆండ్రాయిడ్ ఫోన్ ఉంటే జీపే ద్వారా చెయ్యొచ్చు\"\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "vocab = old_tokenizer.get_vocab()\n",
    "total_vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a70e565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "జ\n",
      "TELUGU LETTER JA\n",
      "ీ\n",
      "TELUGU VOWEL SIGN II\n",
      "ప\n",
      "TELUGU LETTER PA\n",
      "ే\n",
      "TELUGU VOWEL SIGN EE\n"
     ]
    }
   ],
   "source": [
    "test_token = \"జీపే\" #Woman - Sthree\n",
    "for char in test_token:\n",
    "  print (char)\n",
    "  print (unicodedata.name(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d97b89ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'కరెంటు బిల్లుల్ని కూడా సులువుగా ఆండ్రాయిడ్ ఫోన్ ఉంటే జీపే ద్వారా చెయ్యొచ్చు'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = old_tokenizer.convert_tokens_to_ids(tokens)\n",
    "res = old_tokenizer.decode(token_ids)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53cb8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['కరెంటు▁', 'బిల్లుల్ని▁', 'కూడా▁', 'సులువుగా▁', 'ఆ', 'ండ్ర', 'ాయి', 'డ్▁', 'ఫోన్▁', 'ఉంటే▁', 'జీ', 'పే▁', 'ద్వారా▁', 'చెయ్యొచ్చు']\n",
      "Decoded: కరెంటు బిల్లుల్ని కూడా సులువుగా ఆండ్రాయిడ్ ఫోన్ ఉంటే జీపే ద్వారా చెయ్యొచ్చు\n",
      "Original: కరెంటు బిల్లుల్ని కూడా సులువుగా ఆండ్రాయిడ్ ఫోన్ ఉంటే జీపే ద్వారా చెయ్యొచ్చు\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded:\", res)\n",
    "print(\"Original:\", example)\n",
    "print(\"Match:\", res == example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9deb67e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ra-task-tokenizer/tokenizer_config.json',\n",
       " 'ra-task-tokenizer/special_tokens_map.json',\n",
       " 'ra-task-tokenizer/tokenizer.model',\n",
       " 'ra-task-tokenizer/added_tokens.json',\n",
       " 'ra-task-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"ra-task-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87ebd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 233 Telugu tokens from Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0\n",
      "Total Telugu tokens extracted: 237\n",
      "Sample Telugu tokens:\n",
      "  1. '▁సౌకర్య'\n",
      "  2. '▁ఉన్న'\n",
      "  3. 'లాలు'\n",
      "  4. 'కర్య'\n",
      "  5. '▁ప్ర'\n",
      "  6. 'న్ని'\n",
      "  7. '▁మరి'\n",
      "  8. '్రామ'\n",
      "  9. 'స్తు'\n",
      "  10. 'ార్'\n",
      "  11. '▁భూ'\n",
      "  12. '▁చే'\n",
      "  13. 'ించ'\n",
      "  14. 'ారి'\n",
      "  15. '▁నీ'\n",
      "  16. 'త్ర'\n",
      "  17. 'న్న'\n",
      "  18. '▁వి'\n",
      "  19. '▁చె'\n",
      "  20. '▁ము'\n",
      "\n",
      "Testing with: 'యూ.పీ.ఐ అనేది చెల్లింపు యాప్లలో డబ్బు బదిలీ కోసం రూపొందించిన బ్యాంకింగ్ సిస్టమ్'\n",
      "Token IDs: [0, 220, 179, 3, 186, 166, 3, 194, 237, 181, 62, 93, 237, 206, 214, 48, 92, 95, 237, 124, 76, 193, 99, 237, 219, 183, 174, 183, 213, 237, 183, 93, 193, 166, 237, 71, 59, 237, 178, 179, 186, 204, 37, 230, 120, 177, 237, 183, 45, 131, 92, 140, 237, 107, 88, 175, 155, 1]\n",
      "Tokens: ['[BOS]', 'య', 'ూ', '[UNK]', 'ప', 'ీ', '[UNK]', 'ఐ', ' ', 'అ', 'నే', 'ది', ' ', 'చ', 'ె', 'ల్ల', 'ిం', 'పు', ' ', 'యా', 'ప్', 'ల', 'లో', ' ', 'డ', 'బ', '్', 'బ', 'ు', ' ', 'బ', 'ది', 'ల', 'ీ', ' ', 'కో', 'సం', ' ', 'ర', 'ూ', 'ప', 'ొ', 'ంది', 'ం', 'చి', 'న', ' ', 'బ', '్యా', 'ంక', 'ిం', 'గ్', ' ', 'సి', 'స్', 'ట', 'మ్', '[EOS]']\n",
      "Decoded: '[BOS]యూ[UNK]పీ[UNK]ఐ అనేది చెల్లింపు యాప్లలో డబ్బు బదిలీ కోసం రూపొందించిన బ్యాంకింగ్ సిస్టమ్[EOS]'\n",
      "Vocab size: 241\n",
      "\n",
      "Tokenizer saved to: /home/hemanth/GIT_Projects/Telugu-ASR/EDA/telugu_token_tokenizer\n",
      "Loaded tokenizer vocab size: 241\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Telugu Tokenizer for Hugging Face Transformers.\n",
    "\n",
    "This tokenizer is designed specifically for Telugu text processing,\n",
    "extracting Telugu tokens from a pretrained model vocabulary.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class TeluguTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, telugu_tokens: Sequence[str], model_max_length: int, **kwargs):\n",
    "        \"\"\"Telugu tokenizer for Hugging Face transformers.\n",
    "\n",
    "        Args:\n",
    "            telugu_tokens (Sequence[str]): List of Telugu tokens extracted from a pretrained model.\n",
    "                Any token which is not included in this list will be replaced by a special token \n",
    "                called [UNK] with id=3. Following are list of all special tokens with their ids:\n",
    "                    \"[BOS]\": 0\n",
    "                    \"[EOS]\": 1\n",
    "                    \"[PAD]\": 2\n",
    "                    \"[UNK]\": 3\n",
    "                An id (starting at 4) will be assigned to each Telugu token.\n",
    "\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.telugu_tokens = telugu_tokens\n",
    "        self.model_max_length = model_max_length\n",
    "        \n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[EOS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[BOS]\": 0,\n",
    "            \"[EOS]\": 1,\n",
    "            \"[PAD]\": 2,\n",
    "            \"[UNK]\": 3,\n",
    "            **{token: i + 4 for i, token in enumerate(telugu_tokens)},\n",
    "        }\n",
    "        \n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "        \n",
    "        # Create a sorted list of tokens by length (longest first) for better tokenization\n",
    "        self._sorted_tokens = sorted(telugu_tokens, key=len, reverse=True)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            pad_token=pad_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._vocab_str_to_int\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize Telugu text using longest-first matching.\"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(text):\n",
    "            # Try to match the longest possible token first\n",
    "            matched = False\n",
    "            for token in self._sorted_tokens:\n",
    "                if text[i:i+len(token)] == token:\n",
    "                    tokens.append(token)\n",
    "                    i += len(token)\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched:\n",
    "                # If no token matches, add the current character as unknown\n",
    "                tokens.append(text[i])\n",
    "                i += 1\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        eos = [self.eos_token_id]\n",
    "        bos = [self.bos_token_id]\n",
    "        result = bos + token_ids_0 + eos\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + eos\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"telugu_tokens\": self.telugu_tokens,\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"TeluguTokenizer\":\n",
    "        return cls(\n",
    "            telugu_tokens=config[\"telugu_tokens\"],\n",
    "            model_max_length=config[\"model_max_length\"]\n",
    "        )\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(cfg, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file, encoding='utf-8') as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)\n",
    "\n",
    "\n",
    "def extract_telugu_tokens_from_pretrained(model_name: str) -> List[str]:\n",
    "    \"\"\"Extract Telugu tokens from a pretrained tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name or path of the pretrained model\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of Telugu tokens found in the vocabulary\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    \n",
    "    # Telugu Unicode block range\n",
    "    telugu_start = 0x0C00\n",
    "    telugu_end = 0x0C7F\n",
    "    \n",
    "    telugu_tokens = []\n",
    "    \n",
    "    for token, token_id in vocab.items():\n",
    "        # Check if token contains Telugu characters by Unicode range\n",
    "        if any(telugu_start <= ord(char) <= telugu_end for char in token):\n",
    "            telugu_tokens.append(token)\n",
    "        else:\n",
    "            # Additional check using unicodedata for Telugu characters\n",
    "            for char in token:\n",
    "                try:\n",
    "                    name = unicodedata.name(char)\n",
    "                    if \"TELUGU\" in name:\n",
    "                        telugu_tokens.append(token)\n",
    "                        break\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    telugu_tokens.sort(key=len, reverse=True)\n",
    "    \n",
    "    print(f\"Extracted {len(telugu_tokens)} Telugu tokens from {model_name}\")\n",
    "    return telugu_tokens\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Extract Telugu tokens from the pretrained model\n",
    "    model_name = \"Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0\"\n",
    "    telugu_tokens = extract_telugu_tokens_from_pretrained(model_name)\n",
    "    telugu_tokens.extend([\" \", \",\",\"!\", \"'\"])\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Total Telugu tokens extracted: {len(telugu_tokens)}\")\n",
    "    print(\"Sample Telugu tokens:\")\n",
    "    for i, token in enumerate(telugu_tokens[:20]):  # Show first 20 tokens\n",
    "        print(f\"  {i+1}. '{token}'\")\n",
    "    \n",
    "    # Create the Telugu tokenizer\n",
    "    model_max_length = 2048\n",
    "    tokenizer = TeluguTokenizer(telugu_tokens, model_max_length)\n",
    "    \n",
    "    # Test the tokenizer\n",
    "    test_text = \"యూ.పీ.ఐ అనేది చెల్లింపు యాప్లలో డబ్బు బదిలీ కోసం రూపొందించిన బ్యాంకింగ్ సిస్టమ్\"  # \"Hello! How are you?\" in Telugu\n",
    "    \n",
    "    print(f\"\\nTesting with: '{test_text}'\")\n",
    "    tokens = tokenizer(test_text, add_special_tokens=True)\n",
    "    print(f\"Token IDs: {tokens['input_ids']}\")\n",
    "    print(f\"Tokens: {tokenizer.convert_ids_to_tokens(tokens['input_ids'])}\")\n",
    "    print(f\"Decoded: '{tokenizer.decode(tokens['input_ids'])}'\")\n",
    "    print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Save the tokenizer\n",
    "    save_path = \"/home/hemanth/GIT_Projects/Telugu-ASR/EDA/telugu_token_tokenizer\"\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(f\"\\nTokenizer saved to: {save_path}\")\n",
    "    \n",
    "    # Load the tokenizer back\n",
    "    loaded_tokenizer = TeluguTokenizer.from_pretrained(save_path)\n",
    "    print(f\"Loaded tokenizer vocab size: {loaded_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca97b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perfect         | WER: 0.000 | CER: 0.000\n",
      "missing_word    | WER: 0.143 | CER: 0.106\n",
      "spelling_error  | WER: 0.143 | CER: 0.043\n",
      "extra_word      | WER: 0.143 | CER: 0.170\n",
      "shortened       | WER: 0.286 | CER: 0.340\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "import unicodedata\n",
    "\n",
    "# Reference (normalized)\n",
    "ref = \"ఆన్లైన్ ద్వారా నేను కె.వై.సి అప్డేట్ చేయవచ్చా ?\"\n",
    "ref = unicodedata.normalize(\"NFC\", ref)\n",
    "\n",
    "# Some hypotheses\n",
    "hyps = {\n",
    "    \"perfect\": \"ఆన్లైన్ ద్వారా నేను కె.వై.సి అప్డేట్ చేయవచ్చా ?\",\n",
    "    \"missing_word\": \"ఆన్లైన్ ద్వారా కె.వై.సి అప్డేట్ చేయవచ్చా ?\",  # dropped \"నేను\"\n",
    "    \"spelling_error\": \"ఆన్లైన్ దారా నేను కె.వై.సి అప్డేట్ చేయవచ్చా ?\",  # typo \"ద్వారా\"→\"దారా\"\n",
    "    \"extra_word\": \"ఆన్లైన్ ద్వారా నేను ఇప్పుడు కె.వై.సి అప్డేట్ చేయవచ్చా ?\",  # inserted \"ఇప్పుడు\"\n",
    "    \"shortened\": \"ఆన్లైన్ నేను అప్డేట్ చేయవచ్చా ?\",  # dropped multiple words\n",
    "}\n",
    "\n",
    "for name, hyp in hyps.items():\n",
    "    hyp = unicodedata.normalize(\"NFC\", hyp)\n",
    "    wer = jiwer.wer(ref, hyp)\n",
    "    cer = jiwer.cer(ref, hyp)\n",
    "    print(f\"{name:15} | WER: {wer:.3f} | CER: {cer:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
